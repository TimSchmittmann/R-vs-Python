{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "colab_type": "code",
    "id": "24CQJ-Bgl5Ct",
    "outputId": "3f73fc54-457a-4c0a-9063-321efefcb5d9"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EPqKXh4Fl5C2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 964
    },
    "colab_type": "code",
    "id": "mtwNe-JBl5C5",
    "outputId": "a9ca911c-cd8b-4031-e1c8-3a07430731d9"
   },
   "outputs": [],
   "source": [
    "importdf=pd.read_csv('../data/trainingssets/all_emoji_tweets_03_12_18_7_labels_excluded.csv', sep =';', usecols=['tweet_full_text', 'target'])\n",
    "importdf = importdf.dropna()\n",
    "our_tweets=importdf['tweet_full_text'].astype(str).values.tolist()\n",
    "our_targets = importdf['target'].astype(str).values.tolist()\n",
    "our_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "colab_type": "code",
    "id": "08oLF1QUeVEX",
    "outputId": "4add0983-a954-4b01-e0f0-1fd6b991c82b"
   },
   "outputs": [],
   "source": [
    "our_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vm0szuPdl5DA"
   },
   "source": [
    "– Remove stock market tickers like $GE\n",
    "– Remove retweet text “RT”\n",
    "– Remove hyperlinks\n",
    "– Remove hashtags (only the hashtag # and not the word)\n",
    "– Remove stop words like a, and, the, is, are, etc.\n",
    "– Remove emoticons like :), :D, :(, :-), etc.\n",
    "– Remove punctuation like full-stop, comma, exclamation sign, etc.\n",
    "– Convert words to Stem/Base words using Porter Stemming Algorithm. E.g. words like ‘working’, ‘works’, and ‘worked’ will be converted to their base/stem word “work”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ia_FYw6El5DB",
    "outputId": "af4420b1-8fc2-4a62-9dae-d096894e2926"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    " \n",
    "from nltk.corpus import stopwords \n",
    "stopwords_german = stopwords.words('german')\n",
    " \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('german')\n",
    " \n",
    "from nltk.tokenize import TweetTokenizer\n",
    " \n",
    "# Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    " \n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    " \n",
    "# all emoticons (happy + sad)\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    " \n",
    "def clean_tweets(tweet):\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    " \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    " \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/[^\\s]*', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    # replace years with 'ayearzzz'-Token\n",
    "    tweet = re.sub(r'([1-2][0-9]{3})', r'ayearzzz', tweet)\n",
    "    \n",
    "    # replace numbers with 'anumberzzz'-Token, only numbers outside of words\n",
    "    tweet = re.sub(r'(?<![0-9a-zA-Z])[0-9]+(?![0-9a-zA-Z])', r'anumberzzz', tweet)\n",
    " \n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    " \n",
    "    tweets_clean = []    \n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_german and # remove stopwords\n",
    "              word not in emoticons and # remove emoticons\n",
    "                word not in string.punctuation): # remove punctuation\n",
    "            #tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word) # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "    tweets_clean=\" \".join(tweets_clean)\n",
    "    \n",
    "    # remove numbers that were pulled out of words by tokenizer\n",
    "    tweets_clean = re.sub(r'(?<![0-9a-zA-Z])[0-9]+(?![0-9a-zA-Z])', r'', tweets_clean)\n",
    "    \n",
    "    return tweets_clean\n",
    " \n",
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    " \n",
    "# print cleaned tweet\n",
    "print (clean_tweets(custom_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17054
    },
    "colab_type": "code",
    "id": "h_TnWRs6l5DF",
    "outputId": "16132a3a-a17b-48dd-cdaf-bd3b7984cc6c"
   },
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "for i in range(len(our_tweets)):\n",
    "    corpus.append(clean_tweets(our_tweets[i]))\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C6rHtQr09yx4",
    "outputId": "620aed3f-848d-4906-ee43-150f3533bc65"
   },
   "outputs": [],
   "source": [
    "corpus[13:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17034
    },
    "colab_type": "code",
    "id": "b3xsAa4n_peh",
    "outputId": "6f140d6e-51e6-4480-d47d-4134772e9f9f"
   },
   "outputs": [],
   "source": [
    "y=[]\n",
    "for i in range(len(our_targets)):\n",
    "    #Only use first emoji per tweet for now\n",
    "    y.append(our_targets[i].split(',')[0])\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h0Z6cfUQl5DP"
   },
   "source": [
    "- replace numbers with \"a number\"-token?\n",
    "- filter, that deletes non-latin letters?\n",
    "    - might be hackfixed by min_df-parameter\n",
    "- spellingfixes?\n",
    "    - might be hackfixed by min_df-parameter\n",
    "        -very inelegant, because all misspelled tokens are simply discarded\n",
    "- do bold font/ different typefaces just have different asci characters?\n",
    "    - might be hackfixed by min_df-parameter\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_important_features(vectorizer, model, n=5):\n",
    "    index_to_word = {v:k for k,v in vectorizer.vocabulary_.items()}\n",
    "    \n",
    "    # loop for each class\n",
    "    classes ={}\n",
    "    for class_index in range(model.coef_.shape[0]):\n",
    "        word_importances = [(el, index_to_word[i]) for i,el in enumerate(model.coef_[class_index])]\n",
    "        sorted_coeff = sorted(word_importances, key = lambda x : x[0], reverse=True)\n",
    "        tops = sorted(sorted_coeff[:n], key = lambda x : x[0])\n",
    "        bottom = sorted_coeff[-n:]\n",
    "        classes[class_index] = {\n",
    "            'tops':tops,\n",
    "            'bottom':bottom\n",
    "        }\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_important_words_binary_classification(top_scores, top_words, bottom_scores, bottom_words, name):\n",
    "    y_pos = np.arange(len(top_words))\n",
    "    top_pairs = [(a,b) for a,b in zip(top_words, top_scores)]\n",
    "    top_pairs = sorted(top_pairs, key=lambda x: x[1])\n",
    "    \n",
    "    bottom_pairs = [(a,b) for a,b in zip(bottom_words, bottom_scores)]\n",
    "    bottom_pairs = sorted(bottom_pairs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    top_words = [a[0] for a in top_pairs]\n",
    "    top_scores = [a[1] for a in top_pairs]\n",
    "    \n",
    "    bottom_words = [a[0] for a in bottom_pairs]\n",
    "    bottom_scores = [a[1] for a in bottom_pairs]\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 10))  \n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.barh(y_pos,bottom_scores, align='center', alpha=0.5)\n",
    "    plt.title('Irrelevant', fontsize=20)\n",
    "    plt.yticks(y_pos, bottom_words, fontsize=14)\n",
    "    plt.suptitle('Key words', fontsize=16)\n",
    "    plt.xlabel('Importance', fontsize=20)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.barh(y_pos,top_scores, align='center', alpha=0.5)\n",
    "    plt.title('Disaster', fontsize=20)\n",
    "    plt.yticks(y_pos, top_words, fontsize=14)\n",
    "    plt.suptitle(name, fontsize=16)\n",
    "    plt.xlabel('Importance', fontsize=20)\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "def plot_important_words(importance, class_labels, name):\n",
    "    fig = plt.figure(figsize=(20,200))\n",
    "    for i in range(len(importance)):\n",
    "        top_scores = [a[0] for a in importance[i]['tops']]\n",
    "        top_words = [a[1] for a in importance[i]['tops']]\n",
    "        \n",
    "        y_pos = np.arange(len(top_words))\n",
    "        top_pairs = [(a,b) for a,b in zip(top_words, top_scores)]\n",
    "        top_pairs = sorted(top_pairs, key=lambda x: x[1])\n",
    "    \n",
    "        top_words = [a[0] for a in top_pairs]\n",
    "        top_scores = [a[1] for a in top_pairs]\n",
    "        \n",
    "        subplot = str(int(len(importance)/2)+1)+str(2)+str(i + 1)\n",
    "        plt.subplot(int(len(importance)/2)+1, 2, i + 1)\n",
    "        plt.barh(y_pos,top_scores, align='center', alpha=0.5)\n",
    "        plt.title(class_labels[i], fontsize=20, fontname='Segoe UI Emoji')\n",
    "        plt.yticks(y_pos, top_words, fontsize=14)\n",
    "        plt.suptitle(name, fontsize=16)\n",
    "        plt.xlabel('Importance', fontsize=14)\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.8, hspace=0.6)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start of simple naive bayes evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(corpus, y, test_size=0.4, random_state=0)\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df=0.9, min_df=0.0001, ngram_range: (1, 2))),\n",
    "    ('tfidf', TfidfTransformer(_use_idf: True)),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "text_clf.fit(X_train, y_train)\n",
    "predicted = text_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_dict = metrics.classification_report(y_test, predicted, output_dict=True)\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* White space still there\n",
    "* aFood und anAnimal remarkably good\n",
    "* Copyright ebenfalls? \n",
    "* Rare emojis sometimes pretty good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_f1 = sorted(report_dict.items(), key=lambda kv: float(kv[1]['f1-score']))\n",
    "# Filter lower score labels, the annoying small white-space '' and meta-data\n",
    "filtered_by_f1 = [x for x in sorted_by_f1 if x[1]['f1-score'] >= 0.1 and x[0] != '' \n",
    "                  and x[0] != 'weighted avg' and x[0] != 'micro avg' and x[0] != 'macro avg']\n",
    "filtered_by_f1.reverse()\n",
    "remaining_labels = [x[0] for x in filtered_by_f1]\n",
    "remaining_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of simple NB\n",
    "\n",
    "Try different models on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(corpus, y, test_size=0.4, random_state=0)\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "     'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "     'vect__min_df': [1e-4, 0.1],\n",
    "     'vect__max_df': [0.9, 1.0],\n",
    "     'tfidf__use_idf': (True, False)\n",
    "}\n",
    "gs_clf = GridSearchCV(text_clf, parameters, cv=3, iid=False, n_jobs=2)\n",
    "gs_clf.fit(X_train, y_train)\n",
    "predicted = gs_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best params (score: 0.1337):\n",
    "* tfidf__use_idf: True\n",
    "* vect__max_df: 0.9\n",
    "* vect__min_df: 0.0001\n",
    "* vect__ngram_range: (1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(corpus, y, test_size=0.4, random_state=0)\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                          alpha=1e-3, random_state=0, n_jobs=2,\n",
    "                          max_iter=10, tol=None)),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "     #'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "     #clf__loss': ['hinge', 'log'],\n",
    "     #'clf__penalty': ['l2', 'l1'],\n",
    "     #'clf__alpha': [1e-4, 0.1],\n",
    "     #'vect__max_df': [0.9, 1.0],\n",
    "     #'tfidf__use_idf': (True, False)''\n",
    "}\n",
    "#gs_clf = GridSearchCV(text_clf, parameters, cv=3, iid=False, n_jobs=2)\n",
    "text_clf.fit(X_train, y_train)\n",
    "predicted = text_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best params (score 0.127):\n",
    "* loss: 'hinge'\n",
    "* ngram_range: (1,2)\n",
    "* use_idf: True\n",
    "\n",
    "Others:\n",
    "* loss: 'log' -> score 0.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(corpus, y, test_size=0.4, random_state=0)\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "    ('clf', RandomForestClassifier(n_estimators=500, max_depth=6, n_jobs=2, random_state=0)),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "     #'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "     #'clf__max_depth': [2, 4],\n",
    "     #'clf__penalty': ['l2', 'l1'],\n",
    "     #'clf__alpha': [1e-4, 0.1],\n",
    "     #'vect__max_df': [0.9, 1.0],\n",
    "     #'tfidf__use_idf': (True, False)''\n",
    "}\n",
    "#gs_clf = GridSearchCV(text_clf, parameters, cv=3, iid=False, n_jobs=2)\n",
    "text_clf.fit(X_train, y_train)\n",
    "predicted = text_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best score: 0.0739\n",
    "    * n_estimators: 100\n",
    "    * max_depth: 2\n",
    "    * ngram_range: (1,2)\n",
    "    * use_idf: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of models on all data\n",
    "\n",
    "Start with reduced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y),len(corpus))\n",
    "reduced_corpus = []\n",
    "reduced_y = []\n",
    "for i in range(len(y)):\n",
    "    if y[i] in remaining_labels:\n",
    "        reduced_corpus.append(corpus[i])\n",
    "        reduced_y.append(y[i])\n",
    "print(len(reduced_y),len(reduced_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(reduced_corpus, reduced_y, test_size=0.4, random_state=0)\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(min_df=60)),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "text_clf.fit(X_train, y_train)\n",
    "predicted = text_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_dict = metrics.classification_report(y_test, predicted, output_dict=True)\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "importance = get_most_important_features(text_clf.get_params()['vect'], text_clf.get_params()['clf'], 10)\n",
    "\n",
    "#print(importance[33])\n",
    "\n",
    "#print(text_clf.get_params()['clf'].classes_)\n",
    "#top_scores = [a[0] for a in importance[1]['tops']]\n",
    "#top_words = [a[1] for a in importance[1]['tops']]\n",
    "\n",
    "plot_important_words(importance, text_clf.get_params()['clf'].classes_, \"Most important words for relevance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hohe importance von anumberzzz bei fast allen Labels\n",
    "* Teilweise logische Zuordnungen (Assasins Creed, Fallout, Fußball, Schnee)\n",
    "* Das meiste aber eher nicht nachvollziehbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(reduced_corpus, reduced_y, test_size=0.4, random_state=0)\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(min_df=60)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "     'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "     'tfidf__use_idf': (True, False),\n",
    "     'clf__alpha': (0, 0.1, 1),\n",
    "}\n",
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5, iid=False, n_jobs=3)\n",
    "gs_clf.fit(X_train, y_train)\n",
    "predicted = gs_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best params:\n",
    "* clf__alpha: 0.1\n",
    "* tfidf__use_idf: True\n",
    "* vect__ngram_range: (1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_dict = metrics.classification_report(y_test, predicted, output_dict=True)\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "importance = get_most_important_features(gs_clf.best_estimator_.get_params()['vect'], gs_clf.best_estimator_.get_params()['clf'], 10)\n",
    "plot_important_words(importance, gs_clf.best_estimator_.get_params()['clf'].classes_, \"Most important words for relevance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "main_aktuell_16-11.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
