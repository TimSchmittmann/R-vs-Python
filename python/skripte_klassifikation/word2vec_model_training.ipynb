{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EPqKXh4Fl5C2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import matplotlib.pyplot  as plt\n",
    "import itertools\n",
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1308,
     "status": "ok",
     "timestamp": 1542346489674,
     "user": {
      "displayName": "Elli Lila",
      "photoUrl": "",
      "userId": "07165270434435142571"
     },
     "user_tz": -60
    },
    "id": "ia_FYw6El5DB",
    "outputId": "af4420b1-8fc2-4a62-9dae-d096894e2926"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    " \n",
    "from nltk.corpus import stopwords \n",
    "stopwords_german = stopwords.words('german')\n",
    " \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('german')\n",
    " \n",
    "from nltk.tokenize import TweetTokenizer\n",
    " \n",
    "# Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    " \n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    " \n",
    "# all emoticons (happy + sad)\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    " \n",
    "def clean_tweets(tweet):\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    " \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    " \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/[^\\s]*', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    # replace years with 'ayearzzz'-Token\n",
    "    tweet = re.sub(r'([1-2][0-9]{3})', r'ayearzzz', tweet)\n",
    "    \n",
    "    # replace numbers with 'anumberzzz'-Token, only numbers outside of words\n",
    "    tweet = re.sub(r'(?<![0-9a-zA-Z])[0-9]+(?![0-9a-zA-Z])', r'anumberzzz', tweet)\n",
    " \n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    " \n",
    "    tweets_clean = []    \n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_german and # remove stopwords\n",
    "              word not in emoticons and # remove emoticons\n",
    "                word not in string.punctuation): # remove punctuation\n",
    "            #tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word) # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "    tweets_clean=\" \".join(tweets_clean)\n",
    "    \n",
    "    # remove numbers that were pulled out of words by tokenizer\n",
    "    tweets_clean = re.sub(r'(?<![0-9a-zA-Z])[0-9]+(?![0-9a-zA-Z])', r'', tweets_clean)\n",
    "    \n",
    "    return tweets_clean\n",
    " \n",
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    " \n",
    "# print cleaned tweet\n",
    "print (clean_tweets(custom_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of definitions\n",
    "\n",
    "------------------\n",
    "\n",
    "Start data preparation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 964
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 384,
     "status": "error",
     "timestamp": 1542354212506,
     "user": {
      "displayName": "Heinz RÃ¼hmann",
      "photoUrl": "",
      "userId": "07095926276735907293"
     },
     "user_tz": -60
    },
    "id": "mtwNe-JBl5C5",
    "outputId": "a9ca911c-cd8b-4031-e1c8-3a07430731d9"
   },
   "outputs": [],
   "source": [
    "importdf=pd.read_csv('../data/trainingssets/all_emoji_tweets_03_12_18_7_labels_excluded.csv', sep =';', usecols=['tweet_full_text', 'target'])\n",
    "importdf.dropna(inplace=True)\n",
    "importdf.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_targets = importdf['target'].astype(str).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17034
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 719,
     "status": "ok",
     "timestamp": 1542346758654,
     "user": {
      "displayName": "Elli Lila",
      "photoUrl": "",
      "userId": "07165270434435142571"
     },
     "user_tz": -60
    },
    "id": "b3xsAa4n_peh",
    "outputId": "6f140d6e-51e6-4480-d47d-4134772e9f9f"
   },
   "outputs": [],
   "source": [
    "y=[]\n",
    "for i in range(len(all_targets)):\n",
    "    #Only use first emoji per tweet for now\n",
    "    y.append(all_targets[i].split(',')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_tweets = importdf['tweet_full_text'].astype(str).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17054
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 229938,
     "status": "ok",
     "timestamp": 1542346722963,
     "user": {
      "displayName": "Elli Lila",
      "photoUrl": "",
      "userId": "07165270434435142571"
     },
     "user_tz": -60
    },
    "id": "h_TnWRs6l5DF",
    "outputId": "16132a3a-a17b-48dd-cdaf-bd3b7984cc6c"
   },
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "for i in range(len(y)):\n",
    "    corpus.append(clean_tweets(our_tweets[i]))\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get corpus token lists\n",
    "class CorpusSentenceTokenList:\n",
    "    def __init__(self, corpus):\n",
    "        self.corpus = corpus\n",
    "\n",
    "    def __iter__(self):\n",
    "        for tweet in self.corpus:\n",
    "            yield tweet.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test bigram phrases\n",
    "'''\n",
    "token_list = CorpusTokenList(corpus[0:5])\n",
    "phrases = gensim.models.Phrases(token_list, min_count=1)\n",
    "i = 0\n",
    "for token in token_list:\n",
    "    print(token)\n",
    "    i += 1\n",
    "    if i > 100:\n",
    "        break\n",
    "phrases.add_vocab([['dis', 'is','frankfurt', 'zweit', 'spiel'],[u'the', u'mayor', u'of', u'new', u'york', u'was', u'there'],\n",
    "   [u'machine', u'learning', 'frankfurt', 'zweit', 'spiel', u'can', u'be', u'new', u'york' , u'sometimes']] )\n",
    "for phrase, score in phrases.export_phrases(token_list):\n",
    "    print(phrase, score)\n",
    "    i += 1\n",
    "    if i > 100:\n",
    "        break\n",
    "phrases[['eintracht', 'frankfurt', 'zweit', 'spiel', 'folg', 'anumberzzz', 'anumberzzz', 'anumberzzz', 'spielminut', 'wtf', 'kaum', 'kovac', '...', 'sgeom']]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sentences = CorpusSentenceTokenList(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = gensim.models.Phrases(corpus_sentences, threshold=50, min_count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for phrase, score in phrases.export_phrases(corpus_sentences):\n",
    "    print(phrase,' ',score)\n",
    "    i+=1\n",
    "    if i > 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for tweet in corpus_sentences:\n",
    "    for word in phrases[tweet]:\n",
    "        if '_' in word:\n",
    "            i += 1\n",
    "            print(tweet)\n",
    "            print(phrases[tweet])\n",
    "    if i > 100:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for sentence in phrases[corpus_sentences]:\n",
    "    i+=1\n",
    "    print(sentence)\n",
    "    if i > 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End Data preparation \n",
    "\n",
    "-----------------------------\n",
    "\n",
    "Start word2vec model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model = gensim.models.Word2Vec(\n",
    "    phrases[corpus_sentences],\n",
    "    size=600,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    workers=2,\n",
    "    sg=1,\n",
    "    hs=1,\n",
    "    negative=10,\n",
    "    cbow_mean=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store model\n",
    "model.wv.save_word2vec_format('../data/word2vec/tweets_until_29_11_18_model3.model', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tweets_until_29_11_18_model1.model:\n",
    " * size=300,\n",
    " * window=5,\n",
    " * min_count=20,\n",
    " * workers=2,\n",
    " * sg=1,\n",
    " * hs=1,\n",
    " * negative=10,\n",
    " * cbow_mean=0\n",
    " * phrases incorrect\n",
    " \n",
    "tweets_until_29_11_18_model2.model:\n",
    " * size=500,\n",
    " * window=5,\n",
    " * min_count=1,\n",
    " * workers=2,\n",
    " * sg=1,\n",
    " * hs=1,\n",
    " * negative=10,\n",
    " * cbow_mean=0\n",
    " * phrases incorrect\n",
    " \n",
    "tweets_until_29_11_18_model3.model:\n",
    " * size=600,\n",
    " * window=5,\n",
    " * min_count=5,\n",
    " * workers=2,\n",
    " * sg=1,\n",
    " * hs=1,\n",
    " * negative=10,\n",
    " * cbow_mean=0\n",
    " * phrases correct\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Kopie von bagofwords.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [conda env:venv]",
   "language": "python",
   "name": "conda-env-venv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
