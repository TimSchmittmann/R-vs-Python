{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EPqKXh4Fl5C2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot  as plt\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rc('font',family='Segoe UI Emoji')\n",
    "\n",
    "from sklearn import metrics\n",
    "import itertools\n",
    "\n",
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1308,
     "status": "ok",
     "timestamp": 1542346489674,
     "user": {
      "displayName": "Elli Lila",
      "photoUrl": "",
      "userId": "07165270434435142571"
     },
     "user_tz": -60
    },
    "id": "ia_FYw6El5DB",
    "outputId": "af4420b1-8fc2-4a62-9dae-d096894e2926"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    " \n",
    "from nltk.corpus import stopwords \n",
    "stopwords_german = stopwords.words('german')\n",
    " \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('german')\n",
    " \n",
    "from nltk.tokenize import TweetTokenizer\n",
    " \n",
    "# Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    " \n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    " \n",
    "# all emoticons (happy + sad)\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    " \n",
    "def clean_tweets(tweet):\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    " \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    " \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/[^\\s]*', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    # replace years with 'ayearzzz'-Token\n",
    "    tweet = re.sub(r'([1-2][0-9]{3})', r'ayearzzz', tweet)\n",
    "    \n",
    "    # replace numbers with 'anumberzzz'-Token, only numbers outside of words\n",
    "    tweet = re.sub(r'(?<![0-9a-zA-Z])[0-9]+(?![0-9a-zA-Z])', r'anumberzzz', tweet)\n",
    " \n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    " \n",
    "    tweets_clean = []    \n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_german and # remove stopwords\n",
    "              word not in emoticons and # remove emoticons\n",
    "                word not in string.punctuation): # remove punctuation\n",
    "            #tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word) # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "    tweets_clean=\" \".join(tweets_clean)\n",
    "    \n",
    "    # remove numbers that were pulled out of words by tokenizer\n",
    "    tweets_clean = re.sub(r'(?<![0-9a-zA-Z])[0-9]+(?![0-9a-zA-Z])', r'', tweets_clean)\n",
    "    \n",
    "    return tweets_clean\n",
    " \n",
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    " \n",
    "# print cleaned tweet\n",
    "print (clean_tweets(custom_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_important_features(vectorizer, model, n=5):\n",
    "    index_to_word = {v:k for k,v in vectorizer.vocabulary_.items()}\n",
    "    \n",
    "    # loop for each class\n",
    "    classes ={}\n",
    "    for class_index in range(model.coef_.shape[0]):\n",
    "        word_importances = [(el, index_to_word[i]) for i,el in enumerate(model.coef_[class_index])]\n",
    "        sorted_coeff = sorted(word_importances, key = lambda x : x[0], reverse=True)\n",
    "        tops = sorted(sorted_coeff[:n], key = lambda x : x[0])\n",
    "        bottom = sorted_coeff[-n:]\n",
    "        classes[class_index] = {\n",
    "            'tops':tops,\n",
    "            'bottom':bottom\n",
    "        }\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_important_words_binary(importance, labels, name):\n",
    "    top_scores = [a[0] for a in importance[0]['tops']]\n",
    "    top_words = [a[1] for a in importance[0]['tops']]\n",
    "    \n",
    "    bottom_scores = [a[0] for a in importance[0]['bottom']]\n",
    "    bottom_words = [a[1] for a in importance[0]['bottom']]\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 10))  \n",
    "    y_pos = np.arange(len(top_words))\n",
    "        \n",
    "    plt.subplot(121)\n",
    "    plt.barh(y_pos,bottom_scores, align='center', alpha=0.5)\n",
    "    plt.title(labels[0], fontsize=20)\n",
    "    plt.yticks(y_pos, bottom_words, fontsize=14)\n",
    "    plt.suptitle('Key words', fontsize=16)\n",
    "    plt.xlabel('Importance', fontsize=20)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.barh(y_pos,top_scores, align='center', alpha=0.5)\n",
    "    plt.title(labels[1], fontsize=20)\n",
    "    plt.yticks(y_pos, top_words, fontsize=14)\n",
    "    plt.suptitle(name, fontsize=16)\n",
    "    plt.xlabel('Importance', fontsize=20)\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_important_words_multi_class(importance, class_labels, name):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    for i in range(len(importance)):\n",
    "        top_scores = [a[0] for a in importance[i]['tops']]\n",
    "        top_words = [a[1] for a in importance[i]['tops']]\n",
    "        \n",
    "        y_pos = np.arange(len(top_words))\n",
    "        top_pairs = [(a,b) for a,b in zip(top_words, top_scores)]\n",
    "        top_pairs = sorted(top_pairs, key=lambda x: x[1])\n",
    "    \n",
    "        top_words = [a[0] for a in top_pairs]\n",
    "        top_scores = [a[1] for a in top_pairs]\n",
    "        \n",
    "        subplot = str(int(len(importance)/2)+1)+str(2)+str(i + 1)\n",
    "        plt.subplot(int(len(importance)/2)+1, 2, i + 1)\n",
    "        plt.barh(y_pos,top_scores, align='center', alpha=0.5)\n",
    "        plt.title(class_labels[i], fontsize=20, fontname='Segoe UI Emoji')\n",
    "        plt.yticks(y_pos, top_words, fontsize=14)\n",
    "        plt.suptitle(name, fontsize=16)\n",
    "        plt.xlabel('Importance', fontsize=14)\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.8, hspace=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def plot_LSA(test_data, test_labels, savepath=\"PCA_demo.csv\", plot=True):\n",
    "        lsa = TruncatedSVD(n_components=2)\n",
    "        lsa.fit(test_data)\n",
    "        lsa_scores = lsa.transform(test_data)\n",
    "        label_names = set(test_labels)\n",
    "        color_mapper = {label:idx for idx,label in enumerate(label_names)}\n",
    "        color_column = [color_mapper[label] for label in test_labels]\n",
    "        colors = ['orange','blue']\n",
    "        if plot:\n",
    "            plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=color_column, cmap=mpl.colors.ListedColormap(colors))\n",
    "            red_patch = mpatches.Patch(color='orange', label=list(label_names)[0])\n",
    "            green_patch = mpatches.Patch(color='blue', label=list(label_names)[1])\n",
    "            plt.legend(handles=[red_patch, green_patch], prop={'size': 30})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of definitions\n",
    "\n",
    "------------------\n",
    "\n",
    "Start data preparation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 964
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 384,
     "status": "error",
     "timestamp": 1542354212506,
     "user": {
      "displayName": "Heinz Rühmann",
      "photoUrl": "",
      "userId": "07095926276735907293"
     },
     "user_tz": -60
    },
    "id": "mtwNe-JBl5C5",
    "outputId": "a9ca911c-cd8b-4031-e1c8-3a07430731d9"
   },
   "outputs": [],
   "source": [
    "importdf=pd.read_csv('../data/trainingssets/all_emoji_tweets_03_12_18_7_labels_excluded.csv', sep =';', usecols=['tweet_full_text', 'target'])\n",
    "importdf.dropna(inplace=True)\n",
    "importdf.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_targets = importdf['target'].astype(str).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17034
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 719,
     "status": "ok",
     "timestamp": 1542346758654,
     "user": {
      "displayName": "Elli Lila",
      "photoUrl": "",
      "userId": "07165270434435142571"
     },
     "user_tz": -60
    },
    "id": "b3xsAa4n_peh",
    "outputId": "6f140d6e-51e6-4480-d47d-4134772e9f9f"
   },
   "outputs": [],
   "source": [
    "y=[]\n",
    "for i in range(len(all_targets)):\n",
    "    #Only use first emoji per tweet for now\n",
    "    y.append(all_targets[i].split(',')[0])\n",
    "\n",
    "# for filtering in conversion to binary classification later on\n",
    "dfy=pd.DataFrame(y)\n",
    "dfx=pd.DataFrame(importdf['tweet_full_text'])\n",
    "dfx.columns = range(dfx.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to binary classification\n",
    "binary_labels=['♥️', '😂']   # two Labels chosen for binary classification\n",
    "\n",
    "dfy=dfy[dfy.isin(binary_labels)]\n",
    "dfy.dropna(inplace=True)\n",
    "dfx=dfx[dfy.isin(binary_labels)]\n",
    "dfx.dropna(inplace=True)\n",
    "\n",
    "df=dfx.copy()\n",
    "df.rename(inplace=True, columns={0: \"tweet\"})\n",
    "df['target'] = dfy\n",
    "\n",
    "# balance classes to 50:50 by dropping appropriate (randomized) fraction of majority class\n",
    "majority_class='♥️'\n",
    "class_freq=df['target'].value_counts()\n",
    "df = df.drop(df[df['target'] == majority_class].sample(frac=(1-class_freq[1]/class_freq[0]), random_state=123).index)\n",
    "\n",
    "# prepare data for following steps\n",
    "our_tweets=df['tweet'].astype(str).values.tolist()\n",
    "y=df['target']\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17054
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 229938,
     "status": "ok",
     "timestamp": 1542346722963,
     "user": {
      "displayName": "Elli Lila",
      "photoUrl": "",
      "userId": "07165270434435142571"
     },
     "user_tz": -60
    },
    "id": "h_TnWRs6l5DF",
    "outputId": "16132a3a-a17b-48dd-cdaf-bd3b7984cc6c"
   },
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "for i in range(len(our_tweets)):\n",
    "    corpus.append(clean_tweets(our_tweets[i]))\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End Data preparation \n",
    "\n",
    "-----------------------------\n",
    "\n",
    "Start data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(corpus, y, test_size=0.4, random_state=0)\n",
    "pipe_transformer = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df=0.9, min_df=5, ngram_range=(1,2))),\n",
    "])\n",
    "pipe_transformer.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 16))          \n",
    "plot_LSA(pipe_transformer.transform(X_train), y_train)\n",
    "plt.savefig('../figures/tweets_from_03_12_18/binaryclass/countvectorizer_lsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(corpus, y, test_size=0.4, random_state=0)\n",
    "pipe_transformer = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df=0.9, min_df=5, ngram_range=(1,2))),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "])\n",
    "pipe_transformer.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 16))          \n",
    "plot_LSA(pipe_transformer.transform(X_train), y_train)\n",
    "plt.savefig('../figures/tweets_from_03_12_18/binaryclass/tfidf_lsa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For use after arbitrary GridSearch\n",
    "# Needs to be run twice to work? Probably some mistake here\n",
    "#fig = plt.figure(figsize=(16, 16))          \n",
    "#clf = gs_clf.best_estimator_.steps.pop(1)\n",
    "#plot_LSA(gs_clf.best_estimator_.transform(X_train), y_train)\n",
    "#gs_clf.best_estimator_.steps.append(clf)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End data visualisation\n",
    "\n",
    "-----------------------\n",
    "\n",
    "Start simple MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 442,
     "status": "error",
     "timestamp": 1542346860765,
     "user": {
      "displayName": "Elli Lila",
      "photoUrl": "",
      "userId": "07165270434435142571"
     },
     "user_tz": -60
    },
    "id": "SgFYVnOLl5DK",
    "outputId": "d4924b5f-a962-4be0-e573-e8f7338859e9"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(corpus, y, test_size=0.4, random_state=0)\n",
    "pipe_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(min_df=5)),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "pipe_clf.fit(X_train, y_train)\n",
    "predicted = pipe_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "cnf_matrix = confusion_matrix(y_test, predicted)\n",
    "plot_confusion_matrix(cnf_matrix, classes=binary_labels, normalize=True,\n",
    "                      title='Confusion matrix, with normalization')\n",
    "plt.savefig('../figures/tweets_from_03_12_18/binaryclass/countvectorizer_multinomialnb_confusion_matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "importance = get_most_important_features(pipe_clf.get_params()['vect'], pipe_clf.get_params()['clf'], 10)\n",
    "plot_important_words_binary(importance, pipe_clf.get_params()['clf'].classes_, \"Most important words\")\n",
    "plt.savefig('../figures/tweets_from_03_12_18/binaryclass/countvectorizer_multinomialnb_feature_importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End Simple MultinomialNB\n",
    "\n",
    "----------------------------\n",
    "\n",
    "AB HIER ZELLEN SELEKTIV AUSFÜHREN. Das Trainieren einiger Modelle nimmt enorm viel Zeit in Anspruch.\n",
    "\n",
    "Start Advanced MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1033,
     "status": "error",
     "timestamp": 1542346853151,
     "user": {
      "displayName": "Elli Lila",
      "photoUrl": "",
      "userId": "07165270434435142571"
     },
     "user_tz": -60
    },
    "id": "u3qnPxzp93PX",
    "outputId": "fd5809f2-37fc-4e4a-cbab-e82c3edd0efd"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(corpus, y, test_size=0.4, random_state=0)\n",
    "pipe_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df=0.9, min_df=5, ngram_range=(1,2))),\n",
    "    ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "    ('clf', MultinomialNB(alpha=0.1))\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "     #'vect__min_df': (40,60),\n",
    "     #'vect__max_df': (0.8,0.81),\n",
    "}\n",
    "gs_clf = GridSearchCV(pipe_clf, parameters, cv=3, iid=False, n_jobs=-1)\n",
    "gs_clf.fit(X_train, y_train)\n",
    "predicted = gs_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test runs (File from 22.11.18):\n",
    "\n",
    "Run1: \n",
    "     * min_df = 60,\n",
    "     * 'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "     * 'tfidf__use_idf': (True, False),\n",
    "     * 'clf__alpha': (0, 0.1, 1),\n",
    "     \n",
    "Score: 0.73, ngram_range=(1,1), use_idf=False, alpha=0.1\n",
    "\n",
    "Run2:\n",
    "     * alpha=0.1\n",
    "     * 'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "     * 'vect__min_df': (1, 20, 0.1),\n",
    "     * 'vect__max_df': (0.8, 0.95),\n",
    "     * 'tfidf__use_idf': (True, False)\n",
    "\n",
    "Score: 0.7741, ngram_range=(1,2), use_idf=False, max_df=0.8, min_df=1\n",
    "\n",
    "Run3:\n",
    "     * alpha=0.1\n",
    "     * ngram_range=(1, 2),\n",
    "     * min_df=1,\n",
    "     * 'vect__max_df': (0.5,0.6,0.7,0.8),\n",
    "     * use_idf=False\n",
    "\n",
    "Score: 0.7741, max_df=0.5\n",
    "\n",
    "Run4:\n",
    "     * alpha=0.1\n",
    "     * ngram_range=(1, 2),\n",
    "     * 'vect__min_df': (60, 100, 0.01, 0.1),\n",
    "     * 'vect__max_df': (0.5, 0.8),\n",
    "     * use_idf=False\n",
    "\n",
    "Score: 0.733, max_df=0.5, min_df=60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "cnf_matrix = confusion_matrix(y_test, predicted)\n",
    "plot_confusion_matrix(cnf_matrix, classes=binary_labels, normalize=True,\n",
    "                      title='Confusion matrix, with normalization')\n",
    "plt.savefig('../figures/tweets_from_03_12_18/binaryclass/tfidftransformer_multinomialnb_confusion_matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "importance = get_most_important_features(gs_clf.best_estimator_.get_params()['vect'], gs_clf.best_estimator_.get_params()['clf'], 10)\n",
    "plot_important_words_binary(importance, gs_clf.best_estimator_.get_params()['clf'].classes_, \"Most important words\")\n",
    "plt.savefig('../figures/tweets_from_03_12_18/binaryclass/tfidftransformer_multinomialnb_feature_importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End Advanced MultinomialNB\n",
    "\n",
    "----------------------------\n",
    "\n",
    "Start Advanced SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(corpus, y, test_size=0.4, random_state=0)\n",
    "pipe_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df=0.9, min_df=5, ngram_range=(1,2))),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "    ('clf', SGDClassifier(random_state=0, \n",
    "                          max_iter=100, tol=None))\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "     'clf__loss': ['modified_huber', 'log'],\n",
    "     'clf__penalty': ['elasticnet', 'l2'],\n",
    "     'clf__alpha': [1e-5],\n",
    "     'clf__epsilon': [0.01],\n",
    "     'clf__learning_rate': ['invscaling', 'optimal'],\n",
    "     'clf__eta0': [10]\n",
    "     #'clf__eta0': [1e-4, 0.1],\n",
    "}\n",
    "gs_clf = GridSearchCV(pipe_clf, parameters, cv=3, iid=False, n_jobs=-1)\n",
    "gs_clf.fit(X_train, y_train)\n",
    "predicted = gs_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test runs (File from 29.11.18):\n",
    "\n",
    "Run1:\n",
    "     * min_df = 1,\n",
    "     * max_df = 0.9\n",
    "     * max_iter = 100\n",
    "     * tol = None\n",
    "     * ngram_range=(1,2)\n",
    "     * use_idf = True,\n",
    "     * 'clf__loss': ['modified_huber'],\n",
    "     * 'clf__penalty': ['elasticnet'],\n",
    "     * 'clf__alpha': [1e-5],\n",
    "     * 'clf__epsilon': [0.01],\n",
    "     * 'clf__learning_rate': ['invscaling'],\n",
    "     * 'clf__eta0': [10]\n",
    "     \n",
    "Score: 0.786900\n",
    "\n",
    "Test runs (File from 22.11.18):\n",
    "\n",
    "Run1: \n",
    "     * min_df = 1,\n",
    "     * max_df = 0.9\n",
    "     * max_iter = 3\n",
    "     * tol = None\n",
    "     * ngram_range=(1,2)\n",
    "     * 'tfidf__use_idf': (True, False),\n",
    "     * 'clf__loss': ['hinge', 'log'],\n",
    "     * 'clf__penalty': ['l2', 'l1'],\n",
    "     * 'clf__alpha': [1e-4, 0.1],\n",
    "     \n",
    "Score: 0.74145 clf__alpha: 0.0001, loss: 'hinge', penalty: 'l2', use_idf: True\n",
    "\n",
    "Run2: \n",
    "     * min_df = 1,\n",
    "     * max_df = 0.9\n",
    "     * max_iter = 1000\n",
    "     * tol = None\n",
    "     * ngram_range=(1,2)\n",
    "     * use_idf = True,\n",
    "     * loss = 'log',\n",
    "     * penalty = 'l2',\n",
    "     * alpha = 1e-4\n",
    "     \n",
    "Score: 0.7378\n",
    "\n",
    "Run3: \n",
    "     * min_df = 1,\n",
    "     * max_df = 0.9\n",
    "     * max_iter = 100\n",
    "     * tol = None\n",
    "     * ngram_range=(1,2)\n",
    "     * use_idf = True,\n",
    "     * loss = 'log',\n",
    "     * penalty = 'l2',\n",
    "     * 'clf__alpha': [1e-3,1e-2,0.1,1,10]\n",
    "     \n",
    "Score: 0.60, alpha=1e-3\n",
    "\n",
    "Run4: \n",
    "     * min_df = 1,\n",
    "     * max_df = 0.9\n",
    "     * max_iter = 100\n",
    "     * tol = None\n",
    "     * ngram_range=(1,2)\n",
    "     * use_idf = True,\n",
    "     * loss = 'log',\n",
    "     * penalty = 'l2',\n",
    "     * 'clf__alpha': [1e-4, 1e-5, 1e-6],\n",
    "     \n",
    "Score: 0.773987, alpha=1e-5\n",
    "\n",
    "Run5: \n",
    "     * min_df = 1,\n",
    "     * max_df = 0.9\n",
    "     * max_iter = 100\n",
    "     * tol = None\n",
    "     * ngram_range=(1,2)\n",
    "     * use_idf = True,\n",
    "     * 'clf__loss': ['hinge', 'log'],\n",
    "     * 'clf__penalty': ['l2', 'l1'],\n",
    "     * 'clf__alpha': [1e-5],\n",
    "     \n",
    "Score: 0.7733, alpha=1e-05, loss='log', penalty: 'l2'\n",
    "\n",
    "Run6: \n",
    "     * min_df = 1,\n",
    "     * max_df = 0.9\n",
    "     * max_iter = 100\n",
    "     * tol = None\n",
    "     * ngram_range=(1,2)\n",
    "     * use_idf = True,\n",
    "     * 'clf__loss': ['hinge', 'log'],\n",
    "     * 'clf__penalty': ['l2', 'elasticnet'],\n",
    "     * 'clf__alpha': [5e-5, 1e-5, 5e-6],\n",
    "     \n",
    "Score: 0.7761, alpha=5e-06, loss='log', penalty='l2'\n",
    "\n",
    "Run7: \n",
    "     * min_df = 1,\n",
    "     * max_df = 0.9\n",
    "     * max_iter = 100\n",
    "     * tol = None\n",
    "     * ngram_range=(1,2)\n",
    "     * use_idf = True,\n",
    "     * 'clf__loss': ['modified_huber'],\n",
    "     * 'clf__penalty': ['l2', 'elasticnet'],\n",
    "     * 'clf__alpha': [1e-5, 5e-6],\n",
    "     * 'clf__epsilon': [0.01, 0.1, 1],\n",
    "     \n",
    "Score: 0.773966, alpha=1e-05, epsilon=0.01, penalty='elasticnet'\n",
    "\n",
    "Run8: \n",
    "     * min_df = 1,\n",
    "     * max_df = 0.9\n",
    "     * max_iter = 100\n",
    "     * tol = None\n",
    "     * ngram_range=(1,2)\n",
    "     * use_idf = True,\n",
    "     * 'clf__loss': ['modified_huber','log'],\n",
    "     * 'clf__penalty': ['l2', 'elasticnet'],\n",
    "     * 'clf__alpha': [1e-5],\n",
    "     * 'clf__epsilon': [0.01, 0.001],\n",
    "     * 'clf__learning_rate': ['invscaling', 'adaptive'],\n",
    "     * 'clf__eta0': [10, 1, 0.1]\n",
    "     \n",
    "Score: 0.7765, epsilon=0.01, eta0=10, learning_rate='invscaling', loss='modified_huber', penalty='elasticnet'\n",
    "\n",
    "Run9:\n",
    "     * min_df = 1,\n",
    "     * max_df = 0.9\n",
    "     * max_iter = 100\n",
    "     * tol = None\n",
    "     * ngram_range=(1,2)\n",
    "     * use_idf = True,\n",
    "     * 'clf__loss': ['modified_huber'],\n",
    "     * 'clf__penalty': ['elasticnet'],\n",
    "     * 'clf__alpha': [1e-5],\n",
    "     * 'clf__epsilon': [0.01],\n",
    "     * 'clf__learning_rate': ['invscaling', 'adaptive'],\n",
    "     * 'clf__eta0': [10, 100, 1000]\n",
    "     \n",
    "Score: 0.7765, epsilon=0.01, eta0=10, learning_rate='invscaling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "cnf_matrix = confusion_matrix(y_test, predicted)\n",
    "plot_confusion_matrix(cnf_matrix, classes=binary_labels, normalize=True,\n",
    "                      title='Confusion matrix, with normalization')\n",
    "plt.savefig('../figures/tweets_from_03_12_18/binaryclass/tfidftransformer_sgdclassifier_confusion_matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "importance = get_most_important_features(gs_clf.best_estimator_.get_params()['vect'], gs_clf.best_estimator_.get_params()['clf'], 10)\n",
    "plot_important_words_binary(importance, gs_clf.best_estimator_.get_params()['clf'].classes_, \"Most important words\")\n",
    "plt.savefig('../figures/tweets_from_03_12_18/binaryclass/tfidftransformer_sgdclassifier_feature_importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End Advanced SGDClassifier\n",
    "\n",
    "----------------------------\n",
    "\n",
    "Start Advanced RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(corpus, y, test_size=0.4, random_state=0)\n",
    "pipe_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df=0.9, min_df=5, ngram_range=(1,2))),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "    ('clf', RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=0))\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "     #'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "     #'vect__max_df': [0.9, 1.0],\n",
    "     #'tfidf__use_idf': (True, False)''\n",
    "     #'clf__criterion': ['gini', 'entropy'],\n",
    "     #'clf__max_features': ['log2', 'auto', 0.5],\n",
    "}\n",
    "gs_clf = GridSearchCV(pipe_clf, parameters, cv=3, iid=False, n_jobs=-1)\n",
    "gs_clf.fit(X_train, y_train)\n",
    "predicted = gs_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test runs from tweets until 22.11.2018:\n",
    "\n",
    "Run1:\n",
    "     * min_df = 1,\n",
    "     * max_df = 0.9\n",
    "     * ngram_range=(1,2)\n",
    "     * use_idf = True,\n",
    "     * max_depth = None\n",
    "     * n_estimators=10\n",
    "\n",
    "Score 0.71418\n",
    "\n",
    "Run2:\n",
    "     * min_df = 1,\n",
    "     * max_df = 0.9\n",
    "     * ngram_range=(1,2)\n",
    "     * use_idf = True,\n",
    "     * max_depth = None\n",
    "     * n_estimators=100\n",
    "\n",
    "Score: 0.7478993357500576\n",
    "\n",
    "Run3:\n",
    "     * min_df = 1,\n",
    "     * max_df = 0.9\n",
    "     * ngram_range=(1,2)\n",
    "     * use_idf = True,\n",
    "     * max_depth = None\n",
    "     * n_estimators=1000\n",
    "\n",
    "Score: 0.7520482786077992"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "cnf_matrix = confusion_matrix(y_test, predicted)\n",
    "plot_confusion_matrix(cnf_matrix, classes=binary_labels, normalize=True,\n",
    "                      title='Confusion matrix, with normalization')\n",
    "plt.savefig('../figures/tweets_from_03_12_18/binaryclass/tfidftransformer_randomforestclassifier_confusion_matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End Advanced Random Forest\n",
    "\n",
    "----------------------------\n",
    "\n",
    "Start Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(corpus, y, test_size=0.4, random_state=0)\n",
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df=0.9, min_df=5, ngram_range=(1,2))),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True))\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "     #'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "     #'vect__max_df': [0.9, 1.0],\n",
    "     #'tfidf__use_idf': (True, False)''\n",
    "#     'clf__solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "#     'clf__C': [0.1, 1, 10, 30, 100],\n",
    "}\n",
    "pipe.fit(X_train) \n",
    "gs_clf = LogisticRegressionCV(multi_class='auto', cv=3, n_jobs=-1)\n",
    "gs_clf.fit(pipe.transform(X_train), y_train)\n",
    "predicted = gs_clf.predict(pipe.transform(X_test))\n",
    "np.mean(predicted == y_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test runs (File from 29.11):\n",
    "\n",
    "Run 1:\n",
    " * min_df = 1,\n",
    " * max_df = 0.9\n",
    " * ngram_range=(1,2)\n",
    " * use_idf = True,\n",
    " * multi_class = 'auto\n",
    "\n",
    "score: 0.78506"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "cnf_matrix = confusion_matrix(y_test, predicted)\n",
    "plot_confusion_matrix(cnf_matrix, classes=binary_labels, normalize=True,\n",
    "                      title='Confusion matrix, with normalization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "importance = get_most_important_features(pipe.get_params()['vect'], gs_clf, 10)\n",
    "plot_important_words_binary(importance, gs_clf.classes_, \"Most important words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End Logistic Regression\n",
    "\n",
    "----------------------------\n",
    "\n",
    "Start AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(corpus, y, test_size=0.4, random_state=0)\n",
    "pipe_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df=0.9, min_df=5, ngram_range=(1,2))),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "    ('clf', AdaBoostClassifier(n_estimators=1000))\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "     #'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "     #'vect__max_df': [0.9, 1.0],\n",
    "     #'tfidf__use_idf': (True, False)''\n",
    "     'clf__base_estimator': [DecisionTreeClassifier(max_depth=1)],\n",
    "     #'clf__C': [0.1, 1, 10, 30, 100],\n",
    "}\n",
    "\n",
    "gs_clf = RandomizedSearchCV(pipe_clf, parameters, cv=3, n_jobs=-1)\n",
    "gs_clf.fit(X_train, y_train)\n",
    "predicted = gs_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test runs (File from 29.11.18):\n",
    "    \n",
    "Run1:\n",
    " * min_df = 1,\n",
    " * max_df = 0.9\n",
    " * ngram_range=(1,2)\n",
    " * use_idf = True,\n",
    " * n_estimators=10\n",
    " * clf__base_estimator: [DecisionTreeClassifier(max_depth=1)]\n",
    " \n",
    "Score: 0.694144\n",
    "\n",
    "Run2:\n",
    " * min_df = 1,\n",
    " * max_df = 0.9\n",
    " * ngram_range=(1,2)\n",
    " * use_idf = True,\n",
    " * n_estimators=10\n",
    " * clf__base_estimator: [MultinomialNB(alpha=0.1)]\n",
    "  \n",
    "Score: 0.63089\n",
    "\n",
    "Run3:\n",
    " * min_df = 1,\n",
    " * max_df = 0.9\n",
    " * ngram_range=(1,2)\n",
    " * use_idf = True,\n",
    " * n_estimators=10\n",
    " * clf__base_estimator: [SGDClassifier(random_state=0, loss='modified_huber', penalty='elasticnet',\n",
    "                                alpha=1e-5, epsilon=0.01, learning_rate='invscaling', eta0=10,\n",
    "                                max_iter=100, tol=None)]\n",
    "Score: 0.63\n",
    "\n",
    "Run4:\n",
    " * min_df = 1,\n",
    " * max_df = 0.9\n",
    " * ngram_range=(1,2)\n",
    " * use_idf = True,\n",
    " * n_estimators=10\n",
    " * clf__base_estimator: [SGDClassifier(random_state=0, loss='log', penalty='l2',\n",
    "                                alpha=1e-5)]\n",
    "\n",
    "0.63026\n",
    "\n",
    "Run5:\n",
    " * min_df = 1,\n",
    " * max_df = 0.9\n",
    " * ngram_range=(1,2)\n",
    " * use_idf = True,\n",
    " * n_estimators=1000\n",
    " * clf__base_estimator: [DecisionTreeClassifier(max_depth=1)]\n",
    "\n",
    "Score 0.7532143119535669 (overfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "cnf_matrix = confusion_matrix(y_test, predicted)\n",
    "plot_confusion_matrix(cnf_matrix, classes=binary_labels, normalize=True,\n",
    "                      title='Confusion matrix, with normalization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End AdaBoost\n",
    "\n",
    "----------------------------\n",
    "\n",
    "Start GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(corpus, y, test_size=0.4, random_state=0)\n",
    "pipe_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df=0.9, min_df=5, ngram_range=(1,2))),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "    ('clf', GradientBoostingClassifier(n_estimators=100, random_state=0, verbose=1))\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "     #'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "     #'vect__max_df': [0.9, 1.0],\n",
    "     #'tfidf__use_idf': (True, False)''\n",
    "     'clf__learning_rate': [0.001, 0.01, 0.1, 1],\n",
    "     'clf__max_depth': [1, 3, 5],\n",
    "     'clf__loss' : ['deviance', 'exponential']\n",
    "}\n",
    "gs_clf = RandomizedSearchCV(pipe_clf, parameters, cv=3, n_jobs=-1, verbose=2)\n",
    "gs_clf.fit(X_train, y_train)\n",
    "predicted = gs_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test runs (File from 29.11.2018):\n",
    "\n",
    "Run1:\n",
    " * min_df = 1,\n",
    " * max_df = 0.9\n",
    " * ngram_range=(1,2)\n",
    " * use_idf = True,\n",
    " * n_estimators=100\n",
    " * 'clf__learning_rate': [0.001, 0.01, 0.1, 1],\n",
    " * 'clf__max_depth': [1, 3, 5],\n",
    " * 'clf__loss' : ['deviance', 'exponential']\n",
    " \n",
    " \n",
    "   Iter       Train Loss   Remaining Time \n",
    "         1           1.2279            5.40m\n",
    "         2           1.1986            5.10m\n",
    "         3           1.1803            4.95m\n",
    "         4           1.1670            4.77m\n",
    "         5           1.1555            4.74m\n",
    "         6           1.1466            4.63m\n",
    "         7           1.1363            4.61m\n",
    "         8           1.1267            4.54m\n",
    "         9           1.1186            4.48m\n",
    "        10           1.1115            4.42m\n",
    "        20           1.0570            3.87m\n",
    "        30           1.0211            3.40m\n",
    "        40           0.9952            2.89m\n",
    "        50           0.9714            2.40m\n",
    "        60           0.9509            1.91m\n",
    "        70           0.9332            1.43m\n",
    "        80           0.9178           57.16s\n",
    "        90           0.9044           28.45s\n",
    "       100           0.8921            0.00s\n",
    "       \n",
    "Score: 0.7418999338770113, clf__learning_rate: 1, clf__loss: 'deviance', clf__max_depth: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "cnf_matrix = confusion_matrix(y_test, predicted)\n",
    "plot_confusion_matrix(cnf_matrix, classes=binary_labels, normalize=True,\n",
    "                      title='Confusion matrix, with normalization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Kopie von bagofwords.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [conda env:venv]",
   "language": "python",
   "name": "conda-env-venv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
